{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch as t\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import codecs \n",
    "sys.path.append('./model')\n",
    "# from BiLSTM_CRF import BiLSTM_CRF\n",
    "from resultCal import calculate\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import pdb\n",
    "device = t.device('cuda:4' if t.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./res/text_crf.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-8f8c08255dc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./res/text_crf.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Installed/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Installed/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Installed/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Installed/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Installed/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./res/text_crf.csv' does not exist"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./res/text_crf.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./res/text_crf__no_e.csv')\n",
    "df_base = pd.read_csv('./res/base_text.csv')\n",
    "df_train = pd.read_csv('./data/train.csv')\n",
    "test = ' '.join((df_test.n_crop+df_test.n_disease+df_test.n_medicine).values)\n",
    "train = ' '.join((df_train.n_crop+df_train.n_disease+df_train.n_medicine).values)\n",
    "base = ' '.join((df_base.n_crop+df_base.n_disease+df_base.n_medicine).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "test_list, train_list, base_list = [], [], []\n",
    "for x in re.findall('\\'(.*?)\\'', test):\n",
    "    test_list.append(x)\n",
    "for x in re.findall('\\'(.*?)\\'', train):\n",
    "    train_list.append(x)\n",
    "for x in re.findall('\\'(.*?)\\'', base):\n",
    "    base_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_str = pd.read_csv('./data/train_clean.csv').clean_text.map(lambda x: [i.split('/')[0] for i in x.split(' ')]).values\n",
    "train_set =  set()\n",
    "for i in train_str:\n",
    "    train_set.update(\"\".join(i))\n",
    "train_str_unique = list(train_set)\n",
    "\n",
    "test_str = pd.read_csv('./data/test.csv').text.values\n",
    "test_set =  set()\n",
    "for i in test_str:\n",
    "    test_set.update(\"\".join(i))\n",
    "test_str_unique = list(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "蕾\n",
      "蛀\n",
      "塞\n",
      "钻\n",
      "飘\n",
      "厚\n",
      "扣\n",
      "模\n",
      "迹\n",
      "臺\n",
      "皱\n",
      "园\n",
      "蜂\n",
      "瓶\n",
      "糊\n",
      " \n",
      "豆\n",
      "孔\n",
      "被\n",
      "逐\n",
      "熟\n",
      "拂\n",
      "?\n",
      "捉\n",
      "亚\n",
      "穿\n",
      "酵\n",
      ":\n",
      "噁\n",
      "驳\n",
      "按\n",
      "淀\n",
      "蓝\n",
      "胎\n",
      "他\n",
      "装\n",
      "拌\n",
      "疤\n",
      "来\n",
      "热\n",
      "么\n",
      "铵\n",
      "莎\n",
      "抓\n",
      "味\n",
      "段\n",
      "洁\n",
      "衣\n",
      "相\n",
      "挂\n",
      "尝\n",
      "耗\n",
      "零\n",
      "例\n",
      "茧\n",
      "另\n",
      "浏\n",
      "薊\n",
      "觉\n",
      "狠\n",
      "朗\n",
      "/\n"
     ]
    }
   ],
   "source": [
    "for i in test_str_unique:\n",
    "    if i not in train_str_unique: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['烯酰嘧菌酯',\n",
       " '螺虫螨酯',\n",
       " '腐熟农家肥',\n",
       " '腐植酸叶面肥',\n",
       " '塞菌铜',\n",
       " '穿孔病',\n",
       " '多效唑',\n",
       " '溴螨酯',\n",
       " '细菌性果腐病',\n",
       " '海藻素',\n",
       " '高温障碍',\n",
       " '日烧病']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "different_list = []\n",
    "for i in base_list:\n",
    "    if i not in train_list:\n",
    "        different_list.append(i)\n",
    "different_list = list(set(different_list))\n",
    "different_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['根腐病/n_disease ，可以/n_new 在/p 发病初期/n_disease 用/v （/wkz 1）/wky 多菌灵/n_medicine +福美双/n_medicine +乙蒜素/n_medicine +生根剂/n_medicine （/wkz 2）/wky 恶霉灵/n_medicine +乙蒜素/n_medicine +生根剂/n_medicine （/wkz 3）/wky 枯草芽孢杆菌/n_disease +生根剂/n_medicine （/wkz 4）/wky 精/a 甲/n 咯菌腈/n_medicine +生根剂/n_medicine 灌根/n_new 淋/v 茎/n 处理/vn ']\n",
      "\n",
      "['根/b_disease 腐/m_disease 病/m_disease ，/O 可/O 以/O 在/O 发/b_disease 病/m_disease 初/m_disease 期/m_disease 用/O （/O 1/O ）/O 多/b_medicine 菌/m_medicine 灵/m_medicine +/O 福/b_medicine 美/m_medicine 双/m_medicine +/O 乙/b_medicine 蒜/m_medicine 素/m_medicine +/O 生/b_medicine 根/m_medicine 剂/m_medicine （/O 2/O ）/O 恶/b_medicine 霉/m_medicine 灵/m_medicine +/O 乙/b_medicine 蒜/m_medicine 素/m_medicine +/O 生/b_medicine 根/m_medicine 剂/m_medicine （/O 3/O ）/O 枯/b_disease 草/m_disease 芽/m_disease 孢/m_disease 杆/m_disease 菌/m_disease +/O 生/b_medicine 根/m_medicine 剂/m_medicine （/O 4/O ）/O 精/O 甲/O 咯/b_medicine 菌/m_medicine 腈/m_medicine +/O 生/b_medicine 根/m_medicine 剂/m_medicine 灌/O 根/O 淋/O 茎/O 处/O 理/O']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/train_clean_no_e.csv')\n",
    "print(df[df.id=='689-3'].text.values)\n",
    "print()\n",
    "print(df[df.id=='689-3'].clean_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, device):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "#         print(len(tag_to_ix))\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "#         print(tag_to_ix[START_TAG])\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "        self.device = device\n",
    "\n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros([2, 1, self.hidden_dim // 2]),\n",
    "                weight.new_zeros([2, 1, self.hidden_dim // 2]))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        weights = next(self.parameters())\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = weights.new_full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "#                 print(trans_score.device, emit_score.device, forward_var.device)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1).to(self.device)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(self.device), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "#         feats = feats.cpu()\n",
    "#         print(feats.device)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "#         self.hidden = self.init_hidden()\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len: 2790\n"
     ]
    }
   ],
   "source": [
    "with open('./data/maxlen256.pkl', 'rb') as inp:\n",
    "    word2id = pickle.load(inp)\n",
    "    id2word = pickle.load(inp)\n",
    "    tag2id = pickle.load(inp)\n",
    "    id2tag = pickle.load(inp)\n",
    "    x = pickle.load(inp)\n",
    "    y= pickle.load(inp)\n",
    "print(\"train len:\",len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 200\n",
    "EPOCHS = 5\n",
    "tag2id[START_TAG]=len(tag2id)\n",
    "tag2id[STOP_TAG]=len(tag2id)\n",
    "model = BiLSTM_CRF(len(word2id)+1, tag2id, EMBEDDING_DIM, HIDDEN_DIM, device).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-190-8b9aa432475a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Installed/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Installed/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1, shuffle=True, random_state=2020)\n",
    "for epoch in range(EPOCHS):\n",
    "    index=0\n",
    "    for sentence, tags in zip(x_train,y_train):\n",
    "        index+=1\n",
    "        model.zero_grad()\n",
    "\n",
    "        sentence=torch.tensor(sentence, dtype=torch.long).to(device)\n",
    "        tags = torch.tensor([tag2id[t] for t in tags], dtype=torch.long).to(device)\n",
    "\n",
    "        loss = model.neg_log_likelihood(sentence, tags)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if index%300==0:\n",
    "            print(\"epoch\",epoch,\"index\",index)\n",
    "    entityres=[]\n",
    "    entityall=[]\n",
    "    for sentence, tags in zip(x_val,y_val):\n",
    "        sentence=torch.tensor(sentence, dtype=torch.long).to(device)\n",
    "        score,predict = model(sentence)\n",
    "        entityres = calculate(sentence,predict,id2word,id2tag,entityres)\n",
    "        entityall = calculate(sentence,tags,id2word,id2tag,entityall)\n",
    "    jiaoji = [i for i in entityres if i in entityall]\n",
    "    if len(jiaoji)!=0:\n",
    "        zhun = float(len(jiaoji))/len(entityres)\n",
    "        zhao = float(len(jiaoji))/len(entityall)\n",
    "        print(\"test:\")\n",
    "        print(\"zhun:\", zhun)\n",
    "        print(\"zhao:\", zhao)\n",
    "        print(\"f:\", (2*zhun*zhao)/(zhun+zhao))\n",
    "    else:\n",
    "        print(\"zhun:\",0)\n",
    "    \n",
    "    path_name = \"./pretrained_model/model_\"+str(epoch)+\".pt\"\n",
    "    print(path_name)\n",
    "    torch.save(model, path_name)\n",
    "    print(\"model has been saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './data/train_clean.csv'\n",
    "test_file = './data/test.csv'\n",
    "df = pd.read_csv(train_file)\n",
    "df_test = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = df.clean_text.map(lambda x:len(x.split(' ')))\n",
    "lens_test = df_test.text.map(len)\n",
    "sns.distplot(lens, hist=False, label='train')\n",
    "sns.distplot(lens_test, hist=False, label='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal With datasets  (unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "def get_data(x):\n",
    "    res = ''\n",
    "    for words in x.split(' '):\n",
    "        if '/' not in words:\n",
    "            text_list = [i+'/O' for i in list(words)]\n",
    "            res += \" \".join(text_list)\n",
    "            res += ' '\n",
    "            continue            \n",
    "        words = words.split('/')\n",
    "        text, label = words[0], words[1]\n",
    "        \n",
    "        text_list = list(text)\n",
    "        idx = len(text_list)-1\n",
    "        while idx>=0 and text_list[idx] not in ['+', '，', '%', '·', '％']: idx -= 1\n",
    "        #deal with samples which has %\n",
    "        if idx>=0: \n",
    "            prev_str = \" \".join([i+'/O' for i in text_list[:(idx+1)]])\n",
    "            res += prev_str\n",
    "            res += ' '  \n",
    "        text_list = text_list[(idx+1):]\n",
    "        if label == 'n_medicine': tag = 'medicine'\n",
    "        elif label == 'n_disease': tag = 'disease'\n",
    "        elif label == 'n_crop': tag = 'crop'\n",
    "        else: tag='O'\n",
    "        if tag != 'O':\n",
    "            text_list[1:-1] = [i+'/m_'+tag for i in text_list[1:-1]]\n",
    "            text_list[0] += '/b_'+tag\n",
    "            text_list[-1] += '/e_'+tag\n",
    "            res += \" \".join(text_list)\n",
    "        else:\n",
    "            text_list = [i+'/O' for i in text_list]\n",
    "            res += \" \".join(text_list)\n",
    "        res += ' '\n",
    "    return res.strip()\n",
    "df['clean_text'] = df.text.map(get_data)\n",
    "df.to_csv('./data/train_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2pkl():\n",
    "    datas = list()\n",
    "    labels = list()\n",
    "    linedata=list()\n",
    "    linelabel=list()\n",
    "    tags = set()\n",
    "    tags.add('')\n",
    "    input_data = pd.read_csv('./data/train_clean.csv')['clean_text'].values\n",
    "    for line in input_data:\n",
    "        line = line.split()\n",
    "        linedata=[]\n",
    "        linelabel=[]\n",
    "        numNotO=0\n",
    "        for word in line:\n",
    "            word = word.split('/')\n",
    "            linedata.append(word[0])\n",
    "            linelabel.append(word[1])\n",
    "            tags.add(word[1])\n",
    "            if word[1]!='O':\n",
    "                numNotO+=1\n",
    "        if numNotO!=0:\n",
    "            datas.append(linedata)\n",
    "            labels.append(linelabel)\n",
    "\n",
    "    print(len(datas))\n",
    "    print(len(labels))\n",
    "    all_words = [i for data in datas for i in data]\n",
    "    sr_allwords = pd.Series(all_words)\n",
    "    sr_allwords = sr_allwords.value_counts()\n",
    "    set_words = sr_allwords.index\n",
    "    set_ids = range(1, len(set_words)+1)\n",
    "\n",
    "    \n",
    "    tags = [i for i in tags]\n",
    "    tag_ids = range(len(tags))\n",
    "    word2id = pd.Series(set_ids, index=set_words)\n",
    "    id2word = pd.Series(set_words, index=set_ids)\n",
    "    tag2id = pd.Series(tag_ids, index=tags)\n",
    "    id2tag = pd.Series(tags, index=tag_ids)\n",
    "    word2id[\"unknow\"]=len(word2id)+1\n",
    "    id2word[len(word2id)]=\"unknow\"\n",
    "    print(tag2id)\n",
    "    max_len = 256\n",
    "    def X_padding(words):\n",
    "        ids = list(word2id[words])\n",
    "        if len(ids) >= max_len:  \n",
    "            return ids[:max_len]\n",
    "        ids.extend([0]*(max_len-len(ids))) \n",
    "        return ids\n",
    "\n",
    "    def y_padding(tags):\n",
    "        ids = list(tag2id[tags])\n",
    "        if len(ids) >= max_len: \n",
    "            return ids[:max_len]\n",
    "        ids.extend([0]*(max_len-len(ids))) \n",
    "        return ids\n",
    "    df_data = pd.DataFrame({'words': datas, 'tags': labels}, index=range(len(datas)))\n",
    "    df_data['x'] = df_data['words'].apply(X_padding)\n",
    "    df_data['y'] = df_data['tags'].apply(y_padding)\n",
    "    x = np.asarray(list(df_data['x'].values))\n",
    "    y = np.asarray(list(df_data['y'].values))\n",
    "    \n",
    "    import pickle\n",
    "    import os\n",
    "    with open('./data/maxlen256.pkl', 'wb') as outp:\n",
    "        pickle.dump(word2id, outp)\n",
    "        pickle.dump(id2word, outp)\n",
    "        pickle.dump(tag2id, outp)\n",
    "        pickle.dump(id2tag, outp)\n",
    "        pickle.dump(x, outp)\n",
    "        pickle.dump(y, outp)\n",
    "    print('** Finished saving the data.')\n",
    "data2pkl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>n_crop</th>\n",
       "      <th>n_disease</th>\n",
       "      <th>n_medicine</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-1</td>\n",
       "      <td>疫病危害/n_disease ，建议选用/n_new 68·75%霜霉威/n_medicin...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['疫病危害']</td>\n",
       "      <td>['霜霉威', '菌胺', '烯酰吗啉', '湿性粉剂']</td>\n",
       "      <td>疫/b_disease 病/m_disease 危/m_disease 害/e_diseas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-2</td>\n",
       "      <td>疫病/n_disease 炭疽病/n_disease 为害/v ，用/p 银法利/nrf 和...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['疫病', '炭疽病']</td>\n",
       "      <td>['苯甲溴菌腈']</td>\n",
       "      <td>疫/b_disease 病/e_disease 炭/b_disease 疽/m_diseas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-3</td>\n",
       "      <td>炭疽病/n_disease 危害/v 使用/v 肟菌戊唑醇/n_medicine 或/c 苯...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['炭疽病']</td>\n",
       "      <td>['肟菌戊唑醇', '苯甲丙环唑']</td>\n",
       "      <td>炭/b_disease 疽/m_disease 病/e_disease 危/O 害/O 使/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2-1</td>\n",
       "      <td>高温灼伤/n_disease 或/c 药害/n_new 等/udeng 外力/n 损伤造成/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['高温灼伤']</td>\n",
       "      <td>['碧护']</td>\n",
       "      <td>高/b_disease 温/m_disease 灼/m_disease 伤/e_diseas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2-2</td>\n",
       "      <td>日/ng 灼/vg 斑/n 。/wj 叶面喷施/n_new 碧护/n_medicine 和/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['碧护', '螯合钙']</td>\n",
       "      <td>日/O 灼/O 斑/O 。/O 叶/O 面/O 喷/O 施/O 碧/b_medicine 护...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                               text n_crop  \\\n",
       "0  1-1  疫病危害/n_disease ，建议选用/n_new 68·75%霜霉威/n_medicin...     []   \n",
       "1  1-2  疫病/n_disease 炭疽病/n_disease 为害/v ，用/p 银法利/nrf 和...     []   \n",
       "2  1-3  炭疽病/n_disease 危害/v 使用/v 肟菌戊唑醇/n_medicine 或/c 苯...     []   \n",
       "3  2-1  高温灼伤/n_disease 或/c 药害/n_new 等/udeng 外力/n 损伤造成/...     []   \n",
       "4  2-2  日/ng 灼/vg 斑/n 。/wj 叶面喷施/n_new 碧护/n_medicine 和/...     []   \n",
       "\n",
       "       n_disease                     n_medicine  \\\n",
       "0       ['疫病危害']  ['霜霉威', '菌胺', '烯酰吗啉', '湿性粉剂']   \n",
       "1  ['疫病', '炭疽病']                      ['苯甲溴菌腈']   \n",
       "2        ['炭疽病']             ['肟菌戊唑醇', '苯甲丙环唑']   \n",
       "3       ['高温灼伤']                         ['碧护']   \n",
       "4             []                  ['碧护', '螯合钙']   \n",
       "\n",
       "                                          clean_text  \n",
       "0  疫/b_disease 病/m_disease 危/m_disease 害/e_diseas...  \n",
       "1  疫/b_disease 病/e_disease 炭/b_disease 疽/m_diseas...  \n",
       "2  炭/b_disease 疽/m_disease 病/e_disease 危/O 害/O 使/...  \n",
       "3  高/b_disease 温/m_disease 灼/m_disease 伤/e_diseas...  \n",
       "4  日/O 灼/O 斑/O 。/O 叶/O 面/O 喷/O 施/O 碧/b_medicine 护...  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train_clean.csv')\n",
    "def train_clean_noend(x):\n",
    "    res = []\n",
    "    for i in x.split(' '):\n",
    "        if '/' not in i: continue\n",
    "        try:\n",
    "            text, label = i.split('/')\n",
    "        except:\n",
    "            print(i)\n",
    "        if label[0]=='e': label='m'+label[1:]\n",
    "        res.append(text+'/'+label)\n",
    "    return \" \".join(res)\n",
    "df.clean_text = df.clean_text.map(train_clean_noend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/train_clean_no_e.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open('./data/train.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1-1',\n",
       " 'text': '生理性/n_new 缺钙/n_disease 。/wj 平衡/a 肥水/n 供应/vn ，花/v 前/f 硼/n 花/n 后/f 钙/n 喷雾/n_new 调理/v 。/wj',\n",
       " 'text_list': ['生',\n",
       "  '理',\n",
       "  '性',\n",
       "  '缺',\n",
       "  '钙',\n",
       "  '。',\n",
       "  '平',\n",
       "  '衡',\n",
       "  '肥',\n",
       "  '水',\n",
       "  '供',\n",
       "  '应',\n",
       "  '，',\n",
       "  '花',\n",
       "  '前',\n",
       "  '硼',\n",
       "  '花',\n",
       "  '后',\n",
       "  '钙',\n",
       "  '喷',\n",
       "  '雾',\n",
       "  '调',\n",
       "  '理',\n",
       "  '。'],\n",
       " 'tag_list': ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-dise',\n",
       "  'I-dise',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " 'num_class_entity': 5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
